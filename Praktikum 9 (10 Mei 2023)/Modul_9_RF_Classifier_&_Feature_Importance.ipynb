{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bti47sRPwXaD"
      },
      "source": [
        "<a class=\"anchor\" id=\"0\"></a>\n",
        "# Random Forest Classifier with Feature Importance\n",
        "\n",
        "\n",
        "Hello Praktikum ML class,\n",
        "\n",
        "\n",
        "Random Forest is a supervised machine learning algorithm which is based on ensemble learning. The expected accuracy increases with number of decision-trees in the model. It has demonstrated the **feature selection process** using the Random Forest model to find only the important features, rebuild the model using these features and see its effect on accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLOJsR0vwXaF"
      },
      "source": [
        "## 1. The problem statement <a class=\"anchor\" id=\"1\"></a>\n",
        "\n",
        "In this experiment, We will try to make predictions where the prediction task is to determine whether a person makes over 50K a year. We implement Random Forest Classification with Python and Scikit-Learn. So, to answer the question, We build a Random Forest classifier to predict whether a person makes over 50K a year.\n",
        "\n",
        "We will use the income_dtst.csv as dataset for this experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, your **simulation task** is to complete some line of code that hasn't completed yet. \n",
        "\n",
        "Hint: \n",
        "*   Contain error\n",
        "*   Only one line code per block\n",
        "\n"
      ],
      "metadata": {
        "id": "fJJk4SxOXnvN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ-ACnc7wXaG"
      },
      "source": [
        "## 2. Import libraries <a class=\"anchor\" id=\"2\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kj-Ey2uawXaG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "!pip install category-encoders\n",
        "\n",
        "sns.set(style=\"whitegrid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIy8sxNhwXaH"
      },
      "source": [
        "## 3. Import dataset <a class=\"anchor\" id=\"3\"></a>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CqyfmC0wXaI"
      },
      "outputs": [],
      "source": [
        "data = ''\n",
        "\n",
        "df = pd.read_(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQC1KAgkwXaI"
      },
      "source": [
        "## 4. Exploratory data analysis <a class=\"anchor\" id=\"4\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoRJgek2wXaI"
      },
      "source": [
        "### 4.1  View dimensions of dataset <a class=\"anchor\" id=\"4.1\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m9N4iucwXaI"
      },
      "outputs": [],
      "source": [
        "# print the shape\n",
        "print('The shape of the dataset : ', df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7QxUNuuwXaI"
      },
      "source": [
        "We can see that there are 32561 instances and 15 attributes in the data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvAmHOJTwXaI"
      },
      "source": [
        "### 4.2 Preview the dataset <a class=\"anchor\" id=\"4.2\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "lU-r0saSwXaI"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOfn06eHwXaJ"
      },
      "source": [
        "### 4.3 Rename column names <a class=\"anchor\" id=\"4.3\"></a>\n",
        "\n",
        "We can see that the dataset does not have proper column names. The column names contain underscore. We should give proper names to the columns. We will do it as follows:-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4ga5kHPwXaJ"
      },
      "outputs": [],
      "source": [
        "col_names = []\n",
        "\n",
        "df.columns = col_names\n",
        "\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enI74dZXwXaJ"
      },
      "source": [
        "### 4.4 View summary of dataset <a class=\"anchor\" id=\"4.4\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9mR9jYvwXaJ"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDCHGUDIwXaJ"
      },
      "source": [
        "#### Findings\n",
        "\n",
        "- We can see that the dataset contains 9 character variables and 6 numerical variables.\n",
        "\n",
        "- `income` is the target variable.\n",
        "\n",
        "- There are no missing values in the dataset. We will explore this later,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYIRs9MdwXaJ"
      },
      "source": [
        "### 4.5 Check the data types of columns <a class=\"anchor\" id=\"4.5\"></a>\n",
        "\n",
        "- The above `df.info()` command gives us the number of filled values along with the data types of columns.\n",
        "\n",
        "- If we simply want to check the data type of a particular column, we can use the following command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8ymQMzgwXaJ"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4zBeRvvwXaK"
      },
      "source": [
        "### 4.6 View statistical properties of dataset <a class=\"anchor\" id=\"4.6\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJDMTriiwXaK"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6DtauxGwXaK"
      },
      "source": [
        "- The above `df.describe()` command presents statistical properties in vertical form.\n",
        "\n",
        "- If we want to view the statistical properties in horizontal form, we should run the following command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwTKlrzhwXaK"
      },
      "outputs": [],
      "source": [
        "df.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBOKM4dBwXaK"
      },
      "source": [
        "We can see that the above `df.describe().T` command presents statistical properties in horizontal form."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvALONaOwXaK"
      },
      "source": [
        "#### Important points to note\n",
        "\n",
        "\n",
        "- The above command `df.describe()` helps us to view the statistical properties of numerical variables. It excludes character variables.\n",
        "\n",
        "- If we want to view the statistical properties of character variables, we should run the following command -\n",
        "\n",
        "        `df.describe(include=['object'])`\n",
        "\n",
        "- If we want to view the statistical properties of all the variables, we should run the following command -\n",
        "\n",
        "        `df.describe(include='all')`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXbBf5GtwXaL"
      },
      "outputs": [],
      "source": [
        "df.describe(include='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnKWVikbwXaL"
      },
      "source": [
        "### 4.7 Check for missing values <a class=\"anchor\" id=\"4.7\"></a>\n",
        "\n",
        "\n",
        "- In Python missing data is represented by two values:\n",
        "\n",
        "   - **None** : None is a Python singleton object that is often used for missing data in Python code.\n",
        "\n",
        "   - **NaN** : NaN is an acronym for Not a Number. It is a special floating-point value recognized by all systems   that use the standard IEEE floating-point representation.\n",
        "\n",
        "- There are different methods in place on how to detect missing values.\n",
        "\n",
        "\n",
        "#### Pandas isnull() and notnull() functions \n",
        "\n",
        "- Pandas offers two functions to test for missing values - **isnull()** and **notnull()**. \n",
        "\n",
        "- These are simple functions that return a boolean value indicating whether the passed in argument value is in fact missing data.\n",
        "\n",
        "\n",
        "Below, We will list some useful commands to deal with missing values.\n",
        "\n",
        "\n",
        "#### Useful commands to detect missing values \n",
        "\n",
        "- **df.isnull()**\n",
        "\n",
        "The above command checks whether each cell in a dataframe contains missing values or not. If the cell contains missing value, it returns True otherwise it returns False.\n",
        "\n",
        "- **df.isnull().sum()**\n",
        "\n",
        "The above command returns total number of missing values in each column in the dataframe.\n",
        "\n",
        "- **df.isnull().sum().sum()**\n",
        "\n",
        "It returns total number of missing values in the dataframe.\n",
        "\n",
        "\n",
        "- **df.isnull().mean()**\n",
        "\n",
        "It returns percentage of missing values in each column in the dataframe.\n",
        "\n",
        "\n",
        "- **df.isnull().any()**\n",
        "\n",
        "It checks which column has null values and which has not. The columns which has null values returns TRUE and FALSE otherwise.\n",
        "\n",
        "- **df.isnull().any().any()**\n",
        "\n",
        "It returns a boolean value indicating whether the dataframe has missing values or not. If dataframe contains missing values it returns TRUE and FALSE otherwise.\n",
        "\n",
        "- **df.isnull().values.any()**\n",
        "\n",
        "It checks whether a particular column has missing values or not. If the column contains missing values, then it returns TRUE otherwise FALSE.\n",
        "\n",
        "- **df.isnull().values.sum()**\n",
        "\n",
        "It returns the total number of missing values in the dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i40fF9pJwXaL"
      },
      "outputs": [],
      "source": [
        "# check for missing values\n",
        "df..sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1aURMJFwXaL"
      },
      "source": [
        "#### Interpretation\n",
        "\n",
        "We can see that there are no missing values in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-OeswC6wXaL"
      },
      "source": [
        "### 4.8 Check with ASSERT statement <a class=\"anchor\" id=\"4.8\"></a>\n",
        "\n",
        "\n",
        "- We must confirm that our dataset has no missing values.\n",
        "\n",
        "- We can write an **Assert statement** to verify this.\n",
        "\n",
        "- We can use an assert statement to programmatically check that no missing, unexpected 0 or negative values are present.\n",
        "\n",
        "- This gives us confidence that our code is running properly.\n",
        "\n",
        "- **Assert statement** will return nothing if the value being tested is true and will throw an AssertionError if the value is false.\n",
        "\n",
        "- Asserts\n",
        "\n",
        "   - assert 1 == 1 (return Nothing if the value is True)\n",
        "\n",
        "   - assert 1 == 2 (return AssertionError if the value is False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsgZhjd9wXaL"
      },
      "outputs": [],
      "source": [
        "#assert that there are no missing values in the dataframe\n",
        "\n",
        "assert pd.notnull(df).all().all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU7Ya-e9wXaM"
      },
      "source": [
        "#### Interpretation\n",
        "\n",
        "- The above command does not throw any error. Hence, it is confirmed that there are no missing or negative values in the dataset.\n",
        "\n",
        "- All the values are greater than or equal to zero excluding character values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5EtMs_1wXaM"
      },
      "source": [
        "### 4.9 Functional approach to EDA <a class=\"anchor\" id=\"4.9\"></a>\n",
        "\n",
        "- An alternative approach to EDA is to write a function that presents initial EDA of dataset.\n",
        "\n",
        "- We can write such a function as follows :-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdopOFn4wXaM"
      },
      "outputs": [],
      "source": [
        "def initial_eda(df):\n",
        "    if isinstance(df, pd.DataFrame):\n",
        "        total_na = df.isna().sum().sum()\n",
        "        print(\"Dimensions : %d rows, %d columns\" % (df.shape[0], df.shape[1]))\n",
        "        print(\"Total NA Values : %d \" % (total_na))\n",
        "        print(\"%38s %10s     %10s %10s\" % (\"Column Name\", \"Data Type\", \"#Distinct\", \"NA Values\"))\n",
        "        col_name = df.columns\n",
        "        dtyp = df.dtypes\n",
        "        uniq = df.nunique()\n",
        "        na_val = df.isna().sum()\n",
        "        for i in range(len(df.columns)):\n",
        "            print(\"%38s %10s   %10s %10s\" % (col_name[i], dtyp[i], uniq[i], na_val[i]))\n",
        "        \n",
        "    else:\n",
        "        print(\"Expect a DataFrame but got a %15s\" % (type(df)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFSNM8ggwXaM"
      },
      "outputs": [],
      "source": [
        "initial_eda(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSsDoyeywXaf"
      },
      "source": [
        "### Types of variables\n",
        "\n",
        "- In this section, We segregate the dataset into categorical and numerical variables. \n",
        "\n",
        "- There are a mixture of categorical and numerical variables in the dataset. \n",
        "\n",
        "- Categorical variables have data type object. Numerical variables have data type int64.\n",
        "\n",
        "- First of all, We will explore categorical variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRkN5wblwXaf"
      },
      "source": [
        "## 5. Explore Categorical Variables <a class=\"anchor\" id=\"5\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujgC7rbxwXaf"
      },
      "source": [
        "### 5.1 Find categorical variables "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWmUdwUwwXaf"
      },
      "outputs": [],
      "source": [
        "categorical = [var for var in df.columns if df[var].dtype]\n",
        "\n",
        "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
        "\n",
        "print('The categorical variables are :\\n\\n', categorical)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCjGoNkXwXag"
      },
      "source": [
        "### 5.2 Preview categorical variables "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cD3rEvX6wXag"
      },
      "outputs": [],
      "source": [
        "df[].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwseIyCpwXag"
      },
      "source": [
        "### 5.3 Summary of categorical variables \n",
        "\n",
        "- There are 9 categorical variables in the dataset.\n",
        "\n",
        "- The categorical variables are given by `workclass`, `education`, `marital_status`, `occupation`, `relationship`, `race`, `sex`, `native_country` and `income`.\n",
        "\n",
        "- `income` is the target variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LB7mR6bwXag"
      },
      "source": [
        "### 5.4 Frequency distribution of categorical variables \n",
        "\n",
        "Now, we will check the frequency distribution of categorical variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwsdwL9IwXag"
      },
      "outputs": [],
      "source": [
        "for var in categorical: \n",
        "    \n",
        "    print(df[].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FASlmm5PwXag"
      },
      "source": [
        "### 5.5 Percentage of frequency distribution of values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te_w4fLZwXah"
      },
      "outputs": [],
      "source": [
        "for var in categorical:\n",
        "    \n",
        "     print(df[].value_counts()/np.float(len()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqX2PT_owXah"
      },
      "source": [
        "#### Comment\n",
        "\n",
        "- Now, we can see that there are several variables like `workclass`, `occupation` and `native_country` which contain missing values. \n",
        "\n",
        "- Generally, the missing values are coded as `NaN` and python will detect them with the usual command of df.isnull().sum().\n",
        "\n",
        "- But, in this case the missing values are coded as `?`. Python fail to detect these as missing values because it does not consider `?` as missing values. \n",
        "\n",
        "- So, We have to replace `?` with `NaN` so that Python can detect these missing values.\n",
        "\n",
        "- We will explore these variables and replace `?` with `NaN`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBrf4Bh7wXah"
      },
      "source": [
        "### 5.6 Explore the variables "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loOBNR_mwXah"
      },
      "source": [
        "#### Explore `income` target variable "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcOIM5HJwXah"
      },
      "outputs": [],
      "source": [
        "# check for missing values\n",
        "\n",
        "df[''].isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SelA7PAwXah"
      },
      "source": [
        "We can see that there are no missing values in the `income` target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "celvolcbwXah"
      },
      "outputs": [],
      "source": [
        "# view number of unique values\n",
        "\n",
        "df[''].nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxUv-icewXai"
      },
      "source": [
        "There are 2 unique values in the `income` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0i2X5ibbwXai"
      },
      "outputs": [],
      "source": [
        "# view the unique values\n",
        "\n",
        "df[''].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "178iqk4DwXai"
      },
      "source": [
        "The two unique values are `<=50K` and `>50K`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_X3ubPFDwXai"
      },
      "outputs": [],
      "source": [
        "# view the frequency distribution of values\n",
        "\n",
        "df[''].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWznPQ2_wXai"
      },
      "outputs": [],
      "source": [
        "# view percentage of frequency distribution of values\n",
        "\n",
        "df[''].value_counts()/len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cz_wJjKwXai"
      },
      "outputs": [],
      "source": [
        "# visualize frequency distribution of income variable\n",
        "\n",
        "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
        "\n",
        "ax[0] = df['income'].value_counts().plot.pie(explode=[0,0],autopct='%1.1f%%',ax=ax[0],shadow=True)\n",
        "ax[0].set_title('Income Share')\n",
        "\n",
        "\n",
        "#f, ax = plt.subplots(figsize=(6, 8))\n",
        "ax[1] = sns.countplot(x=\"\", data=df, palette=\"Set1\")\n",
        "ax[1].set_title(\"Frequency distribution of income variable\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM8-fFTWwXaj"
      },
      "source": [
        "We can plot the bars horizontally as follows :-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOFKofM0wXaj"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(8, 6))\n",
        "ax = sns.countplot(y=\"income\", data=df, palette=\"Set1\")\n",
        "ax.set_title(\"Frequency distribution of income variable\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bt1J52OwXaj"
      },
      "source": [
        "#### Visualize `income` wrt `sex` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_9H_ScLwXaj"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(10, 8))\n",
        "ax = sns.countplot(x=\"\", hue=\"sex\", data=df, palette=\"Set1\")\n",
        "ax.set_title(\"Frequency distribution of income variable wrt sex\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tftJrXN9wXaj"
      },
      "source": [
        "#### Interpretation\n",
        "\n",
        "\n",
        "- We can see that males make more money than females in both the income categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGwFZkLAwXaj"
      },
      "source": [
        "#### Visualize `income` wrt `race`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WWHdqUTwXaj"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(10, 8))\n",
        "ax = sns.countplot(x=\"income\", hue=\"race\", data=df, palette=\"Set1\")\n",
        "ax.set_title(\"Frequency distribution of income variable wrt race\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnhUbFpBwXak"
      },
      "source": [
        "#### Interpretation\n",
        "\n",
        "\n",
        "- We can see that whites make more money than non-whites in both the income categories."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk6JE14GwXak"
      },
      "source": [
        "#### Explore `workclass` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ix6JMa6qwXak"
      },
      "outputs": [],
      "source": [
        "# check number of unique labels \n",
        "\n",
        "df..nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jduI_hiwXak"
      },
      "outputs": [],
      "source": [
        "# view the unique labels\n",
        "\n",
        "df..unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrTebE0jwXak"
      },
      "outputs": [],
      "source": [
        "# view frequency distribution of values\n",
        "\n",
        "df..value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD8jred_wXak"
      },
      "source": [
        "We can see that there are 1836 values encoded as `?` in workclass variable. We will replace these `?` with `NaN`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2Z3nJSMwXak"
      },
      "outputs": [],
      "source": [
        "# replace '?' values in workclass variable with `NaN`\n",
        "\n",
        "df['workclass'].replace(' ?', np.NaN, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Troe6aPkwXak"
      },
      "outputs": [],
      "source": [
        "# again check the frequency distribution of values in workclass variable\n",
        "\n",
        "df..value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouTEPI82wXal"
      },
      "source": [
        "- Now, we can see that there are no values encoded as `?` in the workclass variable.\n",
        "\n",
        "- We will adopt similar approach with `occupation` and `native_country` column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIHsFmC2wXal"
      },
      "source": [
        "#### Visualize `workclass` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvLwmSiWwXal"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(10, 6))\n",
        "ax = df.workclass.value_counts().plot(kind=\"bar\", color=\"green\")\n",
        "ax.set_title(\"Frequency distribution of workclass variable\")\n",
        "ax.set_xticklabels(df.workclass.value_counts().index, rotation=30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_69v25nnwXal"
      },
      "source": [
        "#### Interpretation\n",
        "\n",
        "\n",
        "- We can see that there are lot more private workers than other category of workers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLdnTnDOwXal"
      },
      "source": [
        "#### Visualize `workclass` variable wrt `income` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vicVh04QwXal"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(12, 8))\n",
        "ax = sns.countplot(x=\"workclass\", hue=\"income\", data=df, palette=\"Set1\")\n",
        "ax.set_title(\"Frequency distribution of workclass variable wrt income\")\n",
        "ax.legend(loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CQbucYjwXal"
      },
      "source": [
        "#### Interpretation\n",
        "\n",
        "\n",
        "- We can see that workers make less than equal to 50k in most of the working categories.\n",
        "\n",
        "- But this trend is more appealing in Private `workclass` category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFE87joXwXam"
      },
      "source": [
        "#### Visualize `workclass` variable wrt `sex` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVwQwc6JwXam"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(12, 8))\n",
        "ax = sns.countplot(x=\"workclass\", hue=\"sex\", data=df, palette=\"Set1\")\n",
        "ax.set_title(\"Frequency distribution of workclass variable wrt sex\")\n",
        "ax.legend(loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EynZdnW5wXam"
      },
      "source": [
        "#### Interpretation\n",
        "\n",
        "\n",
        "- We can see that there are more male workers than female workers in all the working category.\n",
        "\n",
        "- The trend is more appealing in Private sector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6X6O3FhwXam"
      },
      "source": [
        "#### Explore `occupation` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOojBCOrwXam"
      },
      "outputs": [],
      "source": [
        "# check number of unique labels\n",
        "\n",
        "df..nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRoY4csRwXam"
      },
      "outputs": [],
      "source": [
        "# view unique labels\n",
        "\n",
        "df.occupation.()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxfuCyEGwXan"
      },
      "outputs": [],
      "source": [
        "# view frequency distribution of values\n",
        "\n",
        "df.occupation.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEp2WPxJwXan"
      },
      "source": [
        "We can see that there are 1843 values encoded as `?` in occupation variable. We will replace these `?` with `NaN`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYERemrCwXan"
      },
      "outputs": [],
      "source": [
        "# replace '?' values in occupation variable with `NaN`\n",
        "\n",
        "df[''].replace(' ?', np.NaN, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVBBlRkZwXan"
      },
      "outputs": [],
      "source": [
        "# again check the frequency distribution of values\n",
        "\n",
        "df..value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8o9vDcWJwXan"
      },
      "outputs": [],
      "source": [
        "# visualize frequency distribution of `occupation` variable\n",
        "\n",
        "f, ax = plt.subplots(figsize=(12, 8))\n",
        "ax = sns.countplot(x=\"occupation\", data=df, palette=\"Set1\")\n",
        "ax.set_title(\"Frequency distribution of occupation variable\")\n",
        "ax.set_xticklabels(df.occupation.value_counts().index, rotation=30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sREU7lLmwXan"
      },
      "source": [
        "#### Explore `native_country` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5K9m6C9wXan"
      },
      "outputs": [],
      "source": [
        "# check number of unique labels\n",
        "\n",
        "df..nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4Genx6KwXao"
      },
      "outputs": [],
      "source": [
        "# view unique labels \n",
        "\n",
        "df.native_country.unique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16oyFCegwXao"
      },
      "outputs": [],
      "source": [
        "# check frequency distribution of values\n",
        "\n",
        "df.native_country.value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUUjrxxqwXao"
      },
      "source": [
        "We can see that there are 583 values encoded as `?` in native_country variable. We will replace these `?` with `NaN`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JGu5UB8wXao"
      },
      "outputs": [],
      "source": [
        "# replace '?' values in native_country variable with `NaN`\n",
        "\n",
        "df['native_country'].replace(' ?', np.NaN, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUZWYpwUwXao"
      },
      "outputs": [],
      "source": [
        "# again check the frequency distribution of values\n",
        "\n",
        "df.native_country.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bB3I8PrSwXao"
      },
      "outputs": [],
      "source": [
        "# visualize frequency distribution of `native_country` variable\n",
        "\n",
        "f, ax = plt.subplots(figsize=(16, 12))\n",
        "ax = sns.countplot(x=\"native_country\", data=df, palette=\"Set1\")\n",
        "ax.set_title(\"Frequency distribution of native_country variable\")\n",
        "ax.set_xticklabels(df.native_country.value_counts().index, rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjU5zK_QwXao"
      },
      "source": [
        "We can see that `United-States` dominate amongst the `native_country` variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7xt4Q5ywXap"
      },
      "source": [
        "### 5.7 Check missing values in categorical variables "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIeaRmWWwXas"
      },
      "outputs": [],
      "source": [
        "df[categorical].isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APUMUSkbwXas"
      },
      "source": [
        "Now, we can see that `workclass`, `occupation` and `native_country` variable contains missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LktTPOXCwXas"
      },
      "source": [
        "### 5.8 Number of labels: Cardinality \n",
        "\n",
        "- The number of labels within a categorical variable is known as **cardinality**. \n",
        "\n",
        "- A high number of labels within a variable is known as **high cardinality**. \n",
        "\n",
        "- High cardinality may pose some serious problems in the machine learning model. So, We will check for high cardinality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANzml4rBwXas"
      },
      "outputs": [],
      "source": [
        "# check for cardinality in categorical variables\n",
        "\n",
        "for var in categorical:\n",
        "    \n",
        "    print(var, ' contains ', len(df[var].unique()), ' labels')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg9zVkpswXas"
      },
      "source": [
        "We can see that native_country column contains relatively large number of labels as compared to other columns. We will check for cardinality after train-test split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSapMTujwXas"
      },
      "source": [
        "## 6. Explore Numerical Variables <a class=\"anchor\" id=\"6\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9naPZj7qwXat"
      },
      "source": [
        "### 6.1  Find numerical variables "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjyCOOzKwXat"
      },
      "outputs": [],
      "source": [
        "numerical = [var for var in df.columns if df[var].dtype]\n",
        "\n",
        "print('There are {} numerical variables\\n'.format(len(numerical)))\n",
        "\n",
        "print('The numerical variables are :\\n\\n', numerical)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj5Z0qYhwXat"
      },
      "source": [
        "### 6.2 Preview the numerical variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V93Yg0DUwXat"
      },
      "outputs": [],
      "source": [
        "df[].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2fFWyRHwXat"
      },
      "source": [
        "### 6.3 Summary of numerical variables\n",
        "\n",
        "- There are 6 numerical variables.\n",
        "\n",
        "- These are given by `age`, `fnlwgt`, `education_num`,`capital_gain`, `capital_loss` and `hours_per_week`.\n",
        "\n",
        "- All of the numerical variables are of discrete data type."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUhH038HwXat"
      },
      "source": [
        "### 6.4 Check missing values in numerical variables "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-gYXDkhwXat"
      },
      "outputs": [],
      "source": [
        "df[numerical].isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpugEN-UwXau"
      },
      "source": [
        "We can see that there are no missing values in the numerical variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDmR9yXTwXau"
      },
      "source": [
        "### 6.5 Explore numerical variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxmGCnVrwXau"
      },
      "source": [
        "#### Explore `age` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwPstsFSwXau"
      },
      "outputs": [],
      "source": [
        ".nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoXVonPOwXau"
      },
      "source": [
        "#### View the distribution of `age` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjvGR5EVwXau"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(10,8))\n",
        "x = df['age']\n",
        "ax = sns.distplot(x, bins=10, color='blue')\n",
        "ax.set_title(\"Distribution of age variable\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzvIEoI2wXau"
      },
      "source": [
        "We can see that `age` is slightly positively skewed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUI6ILbiwXau"
      },
      "source": [
        "We can use Pandas series object to get an informative axis label as follows :-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDzkkX-_wXav"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(10,8))\n",
        "x = df['age']\n",
        "x = pd.Series(x, name=\"Age variable\")\n",
        "ax = sns.distplot(x, bins=10, color='blue')\n",
        "ax.set_title(\"Distribution of age variable\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x375BgcpwXav"
      },
      "source": [
        "We can shade under the density curve and use a different color as follows:-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyZk52BQwXav"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(10,8))\n",
        "x = df['age']\n",
        "x = pd.Series(x, name=\"Age variable\")\n",
        "ax = sns.kdeplot(x, shade=True, color='red')\n",
        "ax.set_title(\"Distribution of age variable\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68NDEuXKwXav"
      },
      "source": [
        "#### Detect outliers in `age` variable with boxplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyFCd2AzwXav"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(10,8))\n",
        "x = df['age']\n",
        "ax = sns.boxplot(x)\n",
        "ax.set_title(\"Visualize outliers in age variable\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keoaSnO1wXav"
      },
      "source": [
        "We can see that there are lots of outliers in `age` variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auAkAB3AwXav"
      },
      "source": [
        "#### Explore relationship between `age` and `income` variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rslQbAqlwXav"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(10, 8))\n",
        "ax = sns.boxplot(x=\"income\", y=\"age\", data=df)\n",
        "ax.set_title(\"Visualize income wrt age variable\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NmsP4UCwXaw"
      },
      "source": [
        "#### Interpretation\n",
        "\n",
        "- As expected, younger people make less money as compared to senior people."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4K1r9j-wXaw"
      },
      "source": [
        "#### Visualize `income` wrt `age` and `sex` variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ez8dlhomwXaw"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize=(10, 8))\n",
        "ax = sns.boxplot(x=\"\", y=\"age\", hue=\"\", data=df)\n",
        "ax.set_title(\"Visualize income wrt age and sex variable\")\n",
        "ax.legend(loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNXApnCOwXaw"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "ax = sns.catplot(x=\"\", y=\"age\", col=\"\", data=df, kind=\"box\", height=8, aspect=1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk7BMVx4wXaw"
      },
      "source": [
        "#### Interpretation\n",
        "\n",
        "- Senior people make more money than younger people."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OQ5jAuIwXaw"
      },
      "source": [
        "#### Visualize relationship between `race` and `age`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh3h2FcAwXaw"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "sns.boxplot(x ='race', y=\"\", data = df)\n",
        "plt.title(\"Visualize age wrt race\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63HdaVKKwXax"
      },
      "source": [
        "#### Interpretation\n",
        "\n",
        "- Whites are more older than other groups of people."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aS5rBxowXax"
      },
      "source": [
        "#### Find out the correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66snA3AAwXax"
      },
      "outputs": [],
      "source": [
        "# plot correlation heatmap to find out correlations\n",
        "df.corr().style.format(\"{:.4}\").background_gradient(cmap=plt.get_cmap('coolwarm'), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmUDf4FIwXax"
      },
      "source": [
        "#### Interpretation\n",
        "\n",
        "- We can see that there is no strong correlation between variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZB8Is6iwXax"
      },
      "source": [
        "#### Plot pairwise relationships in dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_pslhpgwXax"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(df)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfUCCJo7wXax"
      },
      "source": [
        "#### Interpretation\n",
        "\n",
        "- We can see that `age` and `fnlwgt` are positively skewed.\n",
        "\n",
        "- The variable `education_num` is negatively skewed while `hours_per_week` is normally distributed.\n",
        "\n",
        "- There exists weak positive correlation between `capital_gain` and `education_num` (correlation coefficient=0.1226). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8JVRxf2wXax"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(df, hue=\"income\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa4zgllPwXay"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(df, hue=\"sex\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsvvszCfwXay"
      },
      "source": [
        "## 7. Declare feature vector and target variable <a class=\"anchor\" id=\"7\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h8RDp4VwXay"
      },
      "outputs": [],
      "source": [
        "X = df.drop(['income'], axis=1)\n",
        "\n",
        "y = df['']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuDkBwoVwXay"
      },
      "source": [
        "## 8. Split data into separate training and test set <a class=\"anchor\" id=\"8\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fl2sMyJ-wXay"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = _split(X, y, test_size = 0.3, random_state = 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ascceu1_wXay"
      },
      "outputs": [],
      "source": [
        "# check the shape of X_train and X_test\n",
        "\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_tH0mFBwXay"
      },
      "source": [
        "## 9. Feature Engineering  <a class=\"anchor\" id=\"9\"></a>\n",
        "**Feature Engineering** is the process of transforming raw data into useful features that help us to understand our model better and increase its predictive power. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMQ_xXZJwXay"
      },
      "source": [
        "### 9.1 Display categorical variables in training set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-bhJSz6wXay"
      },
      "outputs": [],
      "source": [
        "categorical = [col for col in X_train. if X_train[col].dtypes ]\n",
        "\n",
        "categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQ9jfCcUwXaz"
      },
      "source": [
        "### 9.2 Display numerical variables in training set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtQJbfAPwXaz"
      },
      "outputs": [],
      "source": [
        "numerical = [col for col in X_train.columns if X_train[col].dtypes != 'O']\n",
        "\n",
        "numerical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3N2Qq6OwXaz"
      },
      "source": [
        "### 9.3 Engineering missing values in categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQ16ov8twXaz"
      },
      "outputs": [],
      "source": [
        "# print percentage of missing values in the categorical variables in training set\n",
        "\n",
        "X_train[categorical].isnull().mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkKynBRgwXaz"
      },
      "outputs": [],
      "source": [
        "# print categorical variables with missing data\n",
        "\n",
        "for col in categorical:\n",
        "    if X_train[col].isnull().mean()>0:\n",
        "        print(col, (X_train[col].isnull().mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQD6glgrwXaz"
      },
      "outputs": [],
      "source": [
        "# impute missing categorical variables with most frequent value\n",
        "\n",
        "for  in [X_train, X_test]:\n",
        "    df2['workclass'].fillna(X_train['workclass'].mode()[0], inplace=True)\n",
        "    df2['occupation'].fillna(X_train['occupation'].mode()[0], inplace=True)\n",
        "    df2['native_country'].fillna(X_train['native_country'].mode()[0], inplace=True)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C97WYRQDwXa0"
      },
      "outputs": [],
      "source": [
        "# check missing values in categorical variables in X_train\n",
        "\n",
        "X_train[categorical].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJTYwv3dwXa0"
      },
      "outputs": [],
      "source": [
        "# check missing values in categorical variables in X_test\n",
        "\n",
        "X_test[categorical].isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPXbOkdEwXa0"
      },
      "source": [
        "As a final check, We will check for missing values in X_train and X_test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVRPqEhIwXa0"
      },
      "outputs": [],
      "source": [
        "# check missing values in X_train\n",
        "\n",
        "X_train.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vdRjDyZwXa0"
      },
      "outputs": [],
      "source": [
        "# check missing values in X_test\n",
        "\n",
        "X_test.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Scx2vuNwXa0"
      },
      "source": [
        "We can see that there are no missing values in X_train and X_test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWtdmeIJwXa0"
      },
      "source": [
        "### 9.4 Encode categorical variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnzJgc1dwXa0"
      },
      "outputs": [],
      "source": [
        "# preview categorical variables in X_train\n",
        "\n",
        "X_train[].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_6ADgaXwXa1"
      },
      "outputs": [],
      "source": [
        "# import category encoders\n",
        "\n",
        "import category_encoders as ce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_d1Co_hwXa1"
      },
      "outputs": [],
      "source": [
        "# encode categorical variables with one-hot encoding\n",
        "\n",
        "encoder = .OneHotEncoder(cols=['workclass', 'education', 'marital_status', 'occupation', 'relationship', \n",
        "                                 'race', 'sex', 'native_country'])\n",
        "\n",
        "X_train = encoder.fit_transform(X_train)\n",
        "\n",
        "X_test = encoder.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyjCmwvjwXa1"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03zd_gZkwXa1"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbrPZGsTwXa1"
      },
      "source": [
        "We can see that from the initial 14 columns, we now have 105 columns in training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEsyJkMhwXa1"
      },
      "source": [
        "Similarly, We will take a look at the X_test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1T6NxTomwXa1"
      },
      "outputs": [],
      "source": [
        "X_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9nLzgcXwXa2"
      },
      "outputs": [],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guzslLkTwXa2"
      },
      "source": [
        "* We now have training and testing set ready for model building. Before that, we should map all the feature variables onto the same scale. It is called **feature scaling**. We will do it as follows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHUP38sYwXa2"
      },
      "source": [
        "## 10. Feature Scaling <a class=\"anchor\" id=\"10\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEZkhIP2wXa2"
      },
      "outputs": [],
      "source": [
        "cols = X_train.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyqE-wo-wXa2"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "X_test = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ3qcxhrwXa2"
      },
      "outputs": [],
      "source": [
        "X_train = pd.DataFrame(X_train, columns=[cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxEZv1rWwXa2"
      },
      "outputs": [],
      "source": [
        "X_test = pd.DataFrame(X_test, columns=[cols])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV0DHcv8wXa2"
      },
      "source": [
        "We now have X_train dataset ready to be fed into the Random Forest classifier. We will do it as follows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0q2sXK6wXa3"
      },
      "source": [
        "## 11. Random Forest Classifier model with default parameters <a class=\"anchor\" id=\"11\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYf5Y56BwXa3"
      },
      "outputs": [],
      "source": [
        "# import Random Forest classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# instantiate the classifier \n",
        "rfc = (random_state=0)\n",
        "\n",
        "# fit the model\n",
        "rfc.fit(X_train, y_train)\n",
        "\n",
        "# Predict the Test set results\n",
        "y_pred = rfc.predict(X_test)\n",
        "\n",
        "# Check accuracy score \n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print('Model accuracy score with 10 decision-trees : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcwcyQ3QwXa3"
      },
      "source": [
        "Here, **y_test** are the true class labels and **y_pred** are the predicted class labels in the test-set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNHNbpDAwXa3"
      },
      "source": [
        "Here, We have build the Random Forest Classifier model with default parameter of `n_estimators = 10`. So, We have used 10 decision-trees to build the model. Now, We will increase the number of decision-trees and see its effect on accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI0T7bupwXa3"
      },
      "source": [
        "## 12. Random Forest Classifier model with 100 Decision Trees  <a class=\"anchor\" id=\"12\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMpf5xt7wXa3"
      },
      "outputs": [],
      "source": [
        "# instantiate the classifier with n_estimators = 10\n",
        "rfc_100 = (n_estimators=100, random_state=0)\n",
        "\n",
        "# fit the model to the training set\n",
        "rfc_100.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set results\n",
        "y_pred_100 = .predict(X_test)\n",
        "\n",
        "# Check accuracy score \n",
        "print('Model accuracy score with 100 decision-trees : {0:0.4f}'. format(accuracy_score(y_test, y_pred_100)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxWtUnZDwXa3"
      },
      "source": [
        "The model accuracy score with 10 decision-trees is 0.8446 but the same with 100 decision-trees is 0.8521. So, as expected accuracy increases with number of decision-trees in the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjHB9Le3wXa4"
      },
      "source": [
        "## 13. Find important features with Random Forest model <a class=\"anchor\" id=\"13\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALbPNLXpwXa4"
      },
      "outputs": [],
      "source": [
        "# create the classifier with n_estimators = 100\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "\n",
        "\n",
        "# fit the model to the training set\n",
        "clf.(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3YAJOKQwXa4"
      },
      "source": [
        "Now, We will use the feature importance variable to see feature importance scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ALcxxg1swXa4"
      },
      "outputs": [],
      "source": [
        "# view the feature scores\n",
        "feature_scores = pd.Series(clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
        "\n",
        "feature_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWoqQoVOwXa5"
      },
      "source": [
        "We can see that the most important feature is `fnlwgt` and least important feature is `native_country_41`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkM7TYD1wXa5"
      },
      "source": [
        "#### Interpretation\n",
        "\n",
        "\n",
        "- The above plot confirms that the most important feature is `fnlwgt` and least important feature is `native_country_41`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXK8xx2IwXa5"
      },
      "source": [
        "## 14. Build the Random Forest model on selected features <a class=\"anchor\" id=\"14\"></a>\n",
        "\n",
        "Now, We will drop the least important feature `native_country_41` from the model, rebuild the model and check its effect on accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbEG1V9NwXa6"
      },
      "outputs": [],
      "source": [
        "# drop the least important feature from X_train and X_test\n",
        "\n",
        "X_train = X_train.drop(['native_country_41'], axis=1)\n",
        "X_test = X_test.drop(['native_country_41'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KGPxzAEwXa6"
      },
      "source": [
        "Now, We will build the random forest model again and check accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtsiaS8YwXa6"
      },
      "outputs": [],
      "source": [
        "# instantiate the classifier with n_estimators = 100\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "\n",
        "# fit the model to the training set\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set results\n",
        "y_pred = clf.(X_test)\n",
        "\n",
        "# Check accuracy score \n",
        "print('Model accuracy score with native_country_41 variable removed : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d1JU8XLwXa6"
      },
      "source": [
        "#### Interpretation\n",
        "\n",
        "- We have removed the `native_country_41` variable from the model, rebuild it and checked its accuracy. \n",
        "\n",
        "- The accuracy of the model now comes out to be 0.8544. \n",
        "\n",
        "- The accuracy of the model with all the variables taken into account is 0.8521. \n",
        "\n",
        "- So, we can see that the model accuracy has been improved with `native_country_41` variable removed from the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bP__z0TwXa6"
      },
      "source": [
        "Now, based on the above analysis we can conclude that our classification model accuracy is very good. Our model is doing a very good job in terms of predicting the class labels.\n",
        "\n",
        "\n",
        "But, it does not give the underlying distribution of values. Also, it does not tell anything about the type of errors our classifer is making. \n",
        "\n",
        "\n",
        "We have another tool called `Confusion matrix` that comes to our rescue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB8z3HuMwXa6"
      },
      "source": [
        "## 15. Confusion matrix <a class=\"anchor\" id=\"15\"></a>\n",
        "\n",
        "A confusion matrix is a tool for summarizing the performance of a classification algorithm. A confusion matrix will give us a clear picture of classification model performance and the types of errors produced by the model. It gives us a summary of correct and incorrect predictions broken down by each category. The summary is represented in a tabular form.\n",
        "\n",
        "\n",
        "Four types of outcomes are possible while evaluating a classification model performance. These four outcomes are described below:-\n",
        "\n",
        "\n",
        "**True Positives (TP)**  True Positives occur when we predict an observation belongs to a certain class and the observation actually belongs to that class.\n",
        "\n",
        "\n",
        "**True Negatives (TN)**  True Negatives occur when we predict an observation does not belong to a certain class and the observation actually does not belong to that class.\n",
        "\n",
        "\n",
        "**False Positives (FP)**  False Positives occur when we predict an observation belongs to a    certain class but the observation actually does not belong to that class. This type of error is called **Type I error.**\n",
        "\n",
        "\n",
        "\n",
        "**False Negatives (FN)**  False Negatives occur when we predict an observation does not belong to a certain class but the observation actually belongs to that class. This is a very serious error and it is called **Type II error.**\n",
        "\n",
        "\n",
        "\n",
        "These four outcomes are summarized in a confusion matrix given below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj2m8FLbwXa7"
      },
      "outputs": [],
      "source": [
        "# Print the Confusion Matrix and slice it into four pieces\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = _matrix(y_test, y_pred)\n",
        "\n",
        "print('Confusion matrix\\n\\n', cm)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0Lc7SzxwXa7"
      },
      "outputs": [],
      "source": [
        "# visualize confusion matrix with seaborn heatmap\n",
        "\n",
        "cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n",
        "                                 index=['Predict Positive:1', 'Predict Negative:0'])\n",
        "\n",
        "sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvOQbPSawXa7"
      },
      "source": [
        "## 16. Classification Report <a class=\"anchor\" id=\"16\"></a>\n",
        "\n",
        "**Classification report** is another way to evaluate the classification model performance. It displays the  **precision**, **recall**, **f1** and **support** scores for the model. We have described these terms in later.\n",
        "\n",
        "We can print a classification report as follows:-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0VD6tb3wXa7"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRRNnq1-wXa7"
      },
      "source": [
        "[Go to Top](#0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tugas Laporan Praktikum Pertemuan 9\n",
        "\n",
        "Cari dataset terkait **time series**, lakukan pemodelan dengan menggunakan Random Forest (Lakukan semua langkah-langkah seperti yang ada pada Simulasi hari ini).\n",
        "\n",
        "*BONUS: Jika terdapat penerapan visualisasi Decision Tree*\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Referensi :  \n",
        "Anda bisa menggunakan link berikut ini sebagai referensi, namun Anda dapat juga dapat mencari referensi lainnya.  \n",
        "https://machinelearningmastery.com/random-forest-for-time-series-forecasting/ \n",
        "\n",
        "\n",
        "Bacaan Lebih lanjut \n",
        "\n",
        "https://machinelearningmastery.com/implement-random-forest-scratch-python/\n",
        "https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/"
      ],
      "metadata": {
        "id": "X5UdErH5SIt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Buat di **notebook yang baru**, ikuti format penamaan file berdasarkan intruksi di Teams."
      ],
      "metadata": {
        "id": "AqJrdiWxSytS"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}